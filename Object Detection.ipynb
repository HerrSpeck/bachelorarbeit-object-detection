{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173a2a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%run Helper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532f99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_legacy(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea7607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_233604) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_169633) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___54382) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_218766) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_209972) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_166209) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_242398) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MODEL = load_model(MODEL_HANDLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load previously generated object detection results from a file, instead of running the object detection use:\n",
    "DETECTED_CLASSES = np.load('outputs/statistics/detections_single.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acdaa484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720ab7d72e714446b376fa01fa6bda60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Inferred: ', max=6990), Label(value='0 / 6990')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Runtime: 260.32 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Load images in batches (to prevent memory error)\n",
    "VALID_PATHS = get_valid_image_paths()\n",
    "TOTAL = len(VALID_PATHS)\n",
    "\n",
    "# How many images have been used so far\n",
    "progress_detection = widgets.IntProgress(min=0, max=TOTAL, description='Inferred: ')\n",
    "progress_label = widgets.Label(value='0 / ' + str(TOTAL))\n",
    "display(widgets.HBox([progress_detection, progress_label]))\n",
    "\n",
    "start_time = time.time()\n",
    "DETECTED_CLASSES = {}\n",
    "CLASSES_TOTALS = {}\n",
    "for current in range(TOTAL):\n",
    "    # Load current image\n",
    "    IMAGE = load_image(valid_paths[current])\n",
    "    \n",
    "    # Setup detections (which can be used to generate statistics later)\n",
    "    IMAGE['detection'] = run_inference_for_image(IMAGE, MODEL)\n",
    "    \n",
    "    # Save detected classes by image\n",
    "    current_detections = get_detections_for_image(IMAGE)\n",
    "    DETECTED_CLASSES.update(current_detections)\n",
    "    \n",
    "    # Sum up amount of detections for each class\n",
    "    for (label, amount) in [(class_label, stats['amount']) for filename, detections in current_detections.items() for class_label, stats in detections.items()]:\n",
    "        if label in CLASSES_TOTALS:\n",
    "            CLASSES_TOTALS[label] += amount\n",
    "        else:\n",
    "            CLASSES_TOTALS[label] = amount\n",
    "    \n",
    "    # Draw detections on images\n",
    "    draw_detections_on_image(IMAGE)\n",
    "    \n",
    "    # Save new images\n",
    "    output = 'outputs/' + MODEL_NAME + '_all/'\n",
    "    save_image(output, IMAGE)\n",
    "\n",
    "    progress_detection.value += 1\n",
    "    progress_label.value = str(progress_detection.value) + ' / ' + str(TOTAL)\n",
    "    \n",
    "runtime = ((time.time() - start_time) / 60)\n",
    "print(\"--- Runtime: \" + f'{runtime:.2f}' + \" minutes ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d881964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average detections per image: 3.66\n"
     ]
    }
   ],
   "source": [
    "# Write the detections to a numpy file, which makes reading the results from the file easier and saves us the trouble\n",
    "# of having to run the object detection every time we want to use it\n",
    "np.save('outputs/statistics/detections_single.npy', DETECTED_CLASSES, allow_pickle=True)\n",
    "\n",
    "generate_detection_chart(DETECTED_CLASSES, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ec70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_LABELS = get_detected_labels(DETECTED_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec968c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get babelnet entities for detected labels\n",
    "get_entity_mapping(DETECTION_LABELS)\n",
    "save_entity_mapping(DETECTION_LABELS, 'outputs/statistics/mapped_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fde536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously generated mappings\n",
    "DETECTION_LABELS = [value for key, value in load_saved_mappings('outputs/statistics/mapped_entities corrected.csv').to_dict('index').items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85d8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RDF information from object detection\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, BNode\n",
    "from rdflib.namespace import FOAF, OWL, RDFS, XSD, SKOS, DCTERMS\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate uri by urlencoding the filename and prepending a namespace, if given\n",
    "def generate_uri(filename, namespace=''):\n",
    "    return URIRef(namespace + parse.quote_plus(filename))\n",
    "\n",
    "# Map files to an aid contained in their filename\n",
    "def map_filename(filename):\n",
    "    # Remove unnecessary prefixes\n",
    "    filename = filename[filename.find('[')+5:filename.find(']')]\n",
    "    \n",
    "    return filename\n",
    "    \n",
    "# Get the url of the mapped wikidata entity as a string (if a mapping exists), returns empty string otherwise\n",
    "def get_mapped_entity(label):\n",
    "    mapped = next((x for x in DETECTION_LABELS if x['class'] == label.capitalize()), None)\n",
    "    \n",
    "    if mapped is None:\n",
    "        return ''\n",
    "    elif mapped['exactMatch']:\n",
    "        return mapped['wikidataid']\n",
    "    else:\n",
    "        return mapped['correctMapping']\n",
    "\n",
    "# Create graph and add namespaces\n",
    "g = Graph()\n",
    "\n",
    "OA = Namespace('http://www.w3.org/ns/oa#')\n",
    "g.namespace_manager.bind('oa', OA)\n",
    "SLOD = Namespace('http://slod.fiz-karlsruhe.de/')\n",
    "g.namespace_manager.bind('slod', SLOD)\n",
    "NIF = Namespace('http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core')\n",
    "g.namespace_manager.bind('nif', NIF)\n",
    "WIKIDATA = Namespace('http://www.wikidata.org/entity/')\n",
    "g.namespace_manager.bind('wd', WIKIDATA)\n",
    "DCMITYPE = Namespace('http://purl.org/dc/dcmitype/')\n",
    "g.namespace_manager.bind('dctypes', DCMITYPE)\n",
    "SCHEMA = Namespace('http://schema.org/')\n",
    "g.namespace_manager.bind('schema', SCHEMA)\n",
    "g.namespace_manager.bind('dcterms', DCTERMS)\n",
    "\n",
    "# Generate date of creation\n",
    "current_date = datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Add Object detector\n",
    "blank_node_detector = BNode(generate_uri('detector'))\n",
    "g.add((blank_node_detector, RDF.type, SLOD.ObjectDetector))\n",
    "g.add((blank_node_detector, RDFS.label, Literal(MODEL_NAME, datatype=XSD.string)))\n",
    "\n",
    "for filename in list(DETECTED_CLASSES.keys()):\n",
    "    # Add image node\n",
    "    depictionId = map_filename(filename)\n",
    "    imageURI = generate_uri(depictionId, SLOD.images + '/slod/')\n",
    "    g.add((imageURI, RDF.type, DCMITYPE.StillImage))\n",
    "    g.add((imageURI, SCHEMA.image, generate_uri(depictionId + '.jpg', SLOD.images + '/slod/')))\n",
    "    g.add((imageURI, DCTERMS.rights, URIRef('https://creativecommons.org/licenses/by/2.0/')))\n",
    "\n",
    "    # Required to keep indices for a single image unique in the graph\n",
    "    offset = 1\n",
    "    # For each detected class on the image\n",
    "    for label, detection in DETECTED_CLASSES[filename].items():\n",
    "        # For each bounding box that exists of the current class in the current image\n",
    "        for index, box in enumerate(detection['boxes']):\n",
    "            # Create two blank nodes for target and selector\n",
    "            blank_node_target = BNode(generate_uri('target-' + depictionId + '-' + str(offset)))\n",
    "            blank_node_selector = BNode(generate_uri('selector-' + depictionId + '-' + str(offset)))\n",
    "            \n",
    "            # Add source for current annotation\n",
    "            g.add((blank_node_target, OA.hasSource, imageURI))\n",
    "            \n",
    "            # Add annotation\n",
    "            annotationURI = generate_uri(depictionId + '-' + str(offset), SLOD.annotations + '/')\n",
    "            g.add((annotationURI, OA.hasTarget, blank_node_target))\n",
    "            g.add((annotationURI, RDF.type, OA.Annotation))\n",
    "            g.add((annotationURI, NIF.confidence, Literal(detection['scores'][index], datatype=XSD.float)))\n",
    "            g.add((annotationURI, RDFS.label, Literal(label.capitalize(), lang='en')))\n",
    "            g.add((annotationURI, DCTERMS.created, Literal(current_date, datatype=XSD.dateTime)))\n",
    "            \n",
    "            # Add link to object detector\n",
    "            g.add((annotationURI, DCTERMS.creator, blank_node_detector))\n",
    "            \n",
    "            # Add wikidata mapping if existent\n",
    "            mapped = get_mapped_entity(label)\n",
    "            if mapped:\n",
    "                g.add((annotationURI, OA.hasBody, generate_uri(mapped, WIKIDATA)))\n",
    "                \n",
    "            # Add Selector\n",
    "            g.add((blank_node_target, OA.hasSelector, blank_node_selector))\n",
    "            g.add((blank_node_selector, RDF.value, Literal('xywh=' + str(box[1]) + ',' + str(box[0]) + ',' + str(box[3]-box[1]) + ',' + str(box[2]-box[0]), datatype=XSD.string)))\n",
    "            g.add((blank_node_selector, RDF.type, OA.FragmentSelector))\n",
    "            g.add((blank_node_selector, DCTERMS.conformsTo, URIRef('http://www.w3.org/TR/media-frags/')))\n",
    "            \n",
    "            # Increase offset\n",
    "            offset += 1\n",
    "\n",
    "# print(g.serialize(format='turtle'))\n",
    "g.serialize(destination='outputs/statistics/rdf.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd24ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
