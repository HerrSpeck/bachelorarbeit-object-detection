{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173a2a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%run Helper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532f99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_legacy(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea7607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_233604) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_169633) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___54382) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_218766) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_209972) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_166209) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_242398) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MODEL = load_model(MODEL_HANDLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load previously generated object detection results from a file, instead of running the object detection use:\n",
    "DETECTED_CLASSES = np.load('outputs/statistics/detections_single.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load valid image paths\n",
    "VALID_PATHS = get_valid_image_paths()\n",
    "TOTAL = len(VALID_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdaa484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515a19d209774be09b203ca8df91f485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Inferred: ', max=6990), Label(value='0 / 6990')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16636/2559049973.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Setup detections (which can be used to generate statistics later)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mIMAGE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'detection'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_inference_for_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMAGE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Save detected classes by image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "# How many images have been used so far\n",
    "progress_detection = widgets.IntProgress(min=0, max=TOTAL, description='Inferred: ')\n",
    "progress_label = widgets.Label(value='0 / ' + str(TOTAL))\n",
    "display(widgets.HBox([progress_detection, progress_label]))\n",
    "\n",
    "start_time = time.time()\n",
    "DETECTED_CLASSES = {}\n",
    "CLASSES_TOTALS = {}\n",
    "for current in range(TOTAL):\n",
    "    # Load current image\n",
    "    IMAGE = load_image(VALID_PATHS[current])\n",
    "    \n",
    "    # Setup detections (which can be used to generate statistics later)\n",
    "    IMAGE['detection'] = run_inference_for_image(IMAGE, MODEL)\n",
    "    \n",
    "    # Save detected classes by image\n",
    "    current_detections = get_detections_for_image(IMAGE)\n",
    "    DETECTED_CLASSES.update(current_detections)\n",
    "    \n",
    "    # Sum up amount of detections for each class\n",
    "    for (label, amount) in [(class_label, stats['amount']) for filename, detections in current_detections.items() for class_label, stats in detections.items()]:\n",
    "        if label in CLASSES_TOTALS:\n",
    "            CLASSES_TOTALS[label] += amount\n",
    "        else:\n",
    "            CLASSES_TOTALS[label] = amount\n",
    "    \n",
    "    # Draw detections on images\n",
    "    draw_detections_on_image(IMAGE)\n",
    "    \n",
    "    # Save new images\n",
    "    output = 'outputs/' + MODEL_NAME + '_all/'\n",
    "    save_image(output, IMAGE)\n",
    "\n",
    "    progress_detection.value += 1\n",
    "    progress_label.value = str(progress_detection.value) + ' / ' + str(TOTAL)\n",
    "    \n",
    "runtime = ((time.time() - start_time) / 60)\n",
    "print(\"--- Runtime: \" + f'{runtime:.2f}' + \" minutes ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c789aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the detections to a numpy file, which makes reading the results from the file easier and saves us the trouble\n",
    "# of having to run the object detection every time we want to use it\n",
    "np.save('outputs/statistics/detections_single.npy', DETECTED_CLASSES, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aff17ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2340/1137324478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mnew_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'boxes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices_075\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mnew_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mDETECTED_075\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'boxes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'height' is not defined"
     ]
    }
   ],
   "source": [
    "DETECTED_075 = {}\n",
    "DETECTED_09 = {}\n",
    "for image, detections in DETECTED_CLASSES.items():\n",
    "    DETECTED_075[image] = {}\n",
    "    DETECTED_09[image] = {}\n",
    "    for label, stats in detections.items():\n",
    "        arr = np.array(stats['scores'])\n",
    "        \n",
    "        indices_075 = []\n",
    "        indices_09 = []\n",
    "        for index, score in enumerate(stats['scores']):\n",
    "            if score >= THRESHOLD:\n",
    "                indices_075.append(index)\n",
    "            if score >= 0.9:\n",
    "                indices_09.append(index)\n",
    "        \n",
    "        filter_arr1 = arr[arr >= THRESHOLD]\n",
    "        if len(filter_arr1) > 0:\n",
    "            DETECTED_075[image][label] = {}\n",
    "            DETECTED_075[image][label]['scores'] = filter_arr1\n",
    "            DETECTED_075[image][label]['amount'] = len(filter_arr1)\n",
    "            new_boxes = [value for index, value in enumerate(stats['boxes']) if index in indices_075]\n",
    "            for index, box in enumerate(new_boxes):\n",
    "                new_boxes[i] = [box[0] * height, box[1] * width, box[2] * height, box[3] * width]\n",
    "            DETECTED_075[image][label]['boxes'] = new_boxes\n",
    "        \n",
    "        filter_arr2 = arr[arr >= 0.9]\n",
    "        if len(filter_arr2) > 0:\n",
    "            DETECTED_09[image][label] = {}\n",
    "            DETECTED_09[image][label]['scores'] = filter_arr2\n",
    "            DETECTED_09[image][label]['amount'] = len(filter_arr2)\n",
    "            DETECTED_09[image][label]['boxes'] = [value for index, value in enumerate(stats['boxes']) if index in indices_09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d881964f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections per image (0.5): 25614(3.6643776824034333)\n",
      "[ 392  986 1486 1167  832  634  402  393  271  180  131   62   24   15\n",
      "    4    5    3    2    1] [1813 1812 1903  867  379  131   64   17    4] [5218 1362  371   34    5]\n",
      "Detections per image (0.75): 10925(1.5629470672389127)\n",
      "Detections per image (0.9): 2226(0.3184549356223176)\n"
     ]
    }
   ],
   "source": [
    "generate_detection_chart(DETECTED_CLASSES, DETECTED_075, DETECTED_09, TOTAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed193eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{E 18 III_RF KB 103} Bild 0023 (KB-Film 103 Aufn. 23) [aid 2-3030335-23].jpg [0.5318922]\n"
     ]
    }
   ],
   "source": [
    "# Print all images that contain a certain object class\n",
    "label = 'Truck'\n",
    "\n",
    "for image, detections in DETECTED_CLASSES.items():\n",
    "    for class_label, stats in detections.items():\n",
    "        if class_label.capitalize() == label:\n",
    "            print(image, stats['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "94ec70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_LABELS = get_detected_labels(DETECTED_075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9ec968c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Person\n",
      "Label: Chair\n",
      "Label: Tie\n",
      "Label: Vase\n",
      "Label: Book\n",
      "Label: Umbrella\n",
      "Label: Potted plant\n",
      "Label: Dog\n",
      "Label: Wine glass\n",
      "Label: Bench\n",
      "Label: Cell phone\n",
      "Label: Clock\n",
      "Label: Bed\n",
      "Label: Bowl\n",
      "Label: Cake\n",
      "Label: Cup\n",
      "Label: Bird\n",
      "Label: Bottle\n",
      "Label: Laptop\n",
      "Label: Suitcase\n",
      "Label: Baseball bat\n",
      "Label: Baseball glove\n",
      "Label: Bicycle\n",
      "Label: Knife\n",
      "Label: Handbag\n",
      "Label: Apple\n",
      "Label: Sports ball\n",
      "Label: Horse\n",
      "Label: Couch\n",
      "Label: Motorcycle\n",
      "Label: Skis\n",
      "Label: Sink\n",
      "Label: Cat\n",
      "Label: Sheep\n",
      "Label: Car\n",
      "Label: Scissors\n"
     ]
    }
   ],
   "source": [
    "# Get babelnet entities for detected labels\n",
    "get_entity_mapping(DETECTION_LABELS)\n",
    "save_entity_mapping(DETECTION_LABELS, 'outputs/statistics/mapped_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fde536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously generated mappings\n",
    "DETECTION_LABELS = [value for key, value in load_saved_mappings('outputs/statistics/mapped_entities corrected.csv').to_dict('index').items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c85d8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RDF information from object detection\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, BNode\n",
    "from rdflib.namespace import FOAF, OWL, RDFS, XSD, SKOS, DCTERMS\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate uri by urlencoding the filename and prepending a namespace, if given\n",
    "def generate_uri(filename, namespace=''):\n",
    "    return URIRef(namespace + parse.quote_plus(filename))\n",
    "\n",
    "# Map files to an aid contained in their filename\n",
    "def map_filename(filename):\n",
    "    # Remove unnecessary prefixes\n",
    "    filename = filename[filename.find('[')+5:filename.find(']')]\n",
    "    \n",
    "    return filename\n",
    "    \n",
    "# Get the url of the mapped wikidata entity as a string (if a mapping exists), returns empty string otherwise\n",
    "def get_mapped_entity(label):\n",
    "    mapped = next((x for x in DETECTION_LABELS if x['class'] == label.capitalize()), None)\n",
    "    \n",
    "    if mapped is None:\n",
    "        return ''\n",
    "    elif mapped['exactMatch']:\n",
    "        return mapped['wikidataid']\n",
    "    else:\n",
    "        return mapped['correctMapping']\n",
    "\n",
    "# Create graph and add namespaces\n",
    "g = Graph()\n",
    "\n",
    "OA = Namespace('http://www.w3.org/ns/oa#')\n",
    "g.namespace_manager.bind('oa', OA)\n",
    "SLOD = Namespace('http://slod.fiz-karlsruhe.de/')\n",
    "g.namespace_manager.bind('slod', SLOD)\n",
    "NIF = Namespace('http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core')\n",
    "g.namespace_manager.bind('nif', NIF)\n",
    "WIKIDATA = Namespace('http://www.wikidata.org/entity/')\n",
    "g.namespace_manager.bind('wd', WIKIDATA)\n",
    "DCMITYPE = Namespace('http://purl.org/dc/dcmitype/')\n",
    "g.namespace_manager.bind('dctypes', DCMITYPE)\n",
    "SCHEMA = Namespace('http://schema.org/')\n",
    "g.namespace_manager.bind('schema', SCHEMA)\n",
    "g.namespace_manager.bind('dcterms', DCTERMS)\n",
    "\n",
    "# Generate date of creation\n",
    "current_date = datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Add Object detector\n",
    "blank_node_detector = BNode(generate_uri('detector'))\n",
    "g.add((blank_node_detector, RDF.type, SLOD.ObjectDetector))\n",
    "g.add((blank_node_detector, RDFS.label, Literal(MODEL_NAME, datatype=XSD.string)))\n",
    "\n",
    "for filename in list(DETECTED_075.keys()):\n",
    "    # Add image node\n",
    "    depictionId = map_filename(filename)\n",
    "    imageURI = generate_uri(depictionId, SLOD.images + '/slod/')\n",
    "    g.add((imageURI, RDF.type, DCMITYPE.StillImage))\n",
    "    g.add((imageURI, SCHEMA.image, generate_uri(depictionId + '.jpg', SLOD.images + '/slod/')))\n",
    "    g.add((imageURI, DCTERMS.rights, URIRef('https://creativecommons.org/licenses/by/2.0/')))\n",
    "\n",
    "    # Required to keep indices for a single image unique in the graph\n",
    "    offset = 1\n",
    "    # For each detected class on the image\n",
    "    for label, detection in DETECTED_075[filename].items():\n",
    "        # For each bounding box that exists of the current class in the current image\n",
    "        for index, box in enumerate(detection['boxes']):\n",
    "            # Create two blank nodes for target and selector\n",
    "            blank_node_target = BNode(generate_uri('target-' + depictionId + '-' + str(offset)))\n",
    "            blank_node_selector = BNode(generate_uri('selector-' + depictionId + '-' + str(offset)))\n",
    "            \n",
    "            # Add source for current annotation\n",
    "            g.add((blank_node_target, OA.hasSource, imageURI))\n",
    "            \n",
    "            # Add annotation\n",
    "            annotationURI = generate_uri(depictionId + '-' + str(offset), SLOD.annotations + '/')\n",
    "            g.add((annotationURI, OA.hasTarget, blank_node_target))\n",
    "            g.add((annotationURI, RDF.type, OA.Annotation))\n",
    "            g.add((annotationURI, NIF.confidence, Literal(detection['scores'][index], datatype=XSD.float)))\n",
    "            g.add((annotationURI, RDFS.label, Literal(label.capitalize(), lang='en')))\n",
    "            g.add((annotationURI, DCTERMS.created, Literal(current_date, datatype=XSD.dateTime)))\n",
    "            \n",
    "            # Add link to object detector\n",
    "            g.add((annotationURI, DCTERMS.creator, blank_node_detector))\n",
    "            \n",
    "            # Add wikidata mapping if existent\n",
    "            mapped = get_mapped_entity(label)\n",
    "            if mapped:\n",
    "                g.add((annotationURI, OA.hasBody, generate_uri(mapped, WIKIDATA)))\n",
    "                \n",
    "            # Add Selector\n",
    "            g.add((blank_node_target, OA.hasSelector, blank_node_selector))\n",
    "            g.add((blank_node_selector, RDF.value, Literal('xywh=' + str(box[1]) + ',' + str(box[0]) + ',' + str(box[3]-box[1]) + ',' + str(box[2]-box[0]), datatype=XSD.string)))\n",
    "            g.add((blank_node_selector, RDF.type, OA.FragmentSelector))\n",
    "            g.add((blank_node_selector, DCTERMS.conformsTo, URIRef('http://www.w3.org/TR/media-frags/')))\n",
    "            \n",
    "            # Increase offset\n",
    "            offset += 1\n",
    "\n",
    "# print(g.serialize(format='turtle'))\n",
    "g.serialize(destination='outputs/statistics/rdf.ttl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
